# 存储虚拟化指南

## 1. 存储虚拟化基础

### 1.1 虚拟化类型
1. 块存储虚拟化
   - LVM
   - iSCSI
   - FC SAN
   - DRBD

2. 文件存储虚拟化
   - NFS
   - GlusterFS
   - Ceph FS
   - HDFS

3. 对象存储虚拟化
   - Ceph RGW
   - Swift
   - MinIO
   - S3

### 1.2 存储架构
1. 本地存储
   - 直接附加存储 (DAS)
   - 本地磁盘阵列
   - 本地 SSD
   - NVMe 设备

2. 网络存储
   - 存储区域网络 (SAN)
   - 网络附加存储 (NAS)
   - 分布式存储
   - 云存储

## 2. 块存储虚拟化

### 2.1 LVM 配置
```bash
# 创建物理卷
pvcreate /dev/sdb /dev/sdc

# 创建卷组
vgcreate vg_data /dev/sdb /dev/sdc

# 创建逻辑卷
lvcreate -L 100G -n lv_data vg_data

# 创建文件系统
mkfs.xfs /dev/vg_data/lv_data

# 挂载配置
echo "/dev/vg_data/lv_data /data xfs defaults 0 0" >> /etc/fstab
mount -a
```

### 2.2 iSCSI 配置
```bash
# 安装 iSCSI 目标端
yum install targetcli

# 配置 iSCSI 目标
targetcli << EOF
cd /backstores/block
create block0 /dev/vg_data/lv_data
cd /iscsi
create iqn.2024-01.com.example:target0
cd /iscsi/iqn.2024-01.com.example:target0/tpg1/luns
create /backstores/block/block0
cd /iscsi/iqn.2024-01.com.example:target0/tpg1/acls
create iqn.2024-01.com.example:initiator0
EOF

# 启动服务
systemctl enable target
systemctl start target

# 配置启动器
yum install iscsi-initiator-utils
echo "InitiatorName=iqn.2024-01.com.example:initiator0" > /etc/iscsi/initiatorname.iscsi
systemctl restart iscsid
iscsiadm -m discovery -t sendtargets -p target_ip
iscsiadm -m node -T iqn.2024-01.com.example:target0 -p target_ip --login
```

### 2.3 DRBD 配置
```bash
# 安装 DRBD
yum install drbd90-utils kmod-drbd90

# 配置 DRBD
cat > /etc/drbd.d/data.res << EOF
resource data {
    protocol C;
    disk {
        on-io-error detach;
    }
    on node1 {
        device /dev/drbd0;
        disk /dev/vg_data/lv_data;
        address 192.168.1.1:7788;
        meta-disk internal;
    }
    on node2 {
        device /dev/drbd0;
        disk /dev/vg_data/lv_data;
        address 192.168.1.2:7788;
        meta-disk internal;
    }
}
EOF

# 初始化 DRBD
drbdadm create-md data
drbdadm up data
drbdadm primary --force data
```

## 3. 文件存储虚拟化

### 3.1 NFS 配置
```bash
# 安装 NFS 服务器
yum install nfs-utils

# 配置导出目录
cat >> /etc/exports << EOF
/data 192.168.1.0/24(rw,sync,no_root_squash)
EOF

# 启动服务
systemctl enable nfs-server
systemctl start nfs-server

# 客户端挂载
mount -t nfs server:/data /mnt
```

### 3.2 GlusterFS 配置
```bash
# 安装 GlusterFS
yum install centos-release-gluster
yum install glusterfs-server

# 启动服务
systemctl enable glusterd
systemctl start glusterd

# 创建信任池
gluster peer probe node2

# 创建卷
gluster volume create data replica 2 \
    node1:/data/brick1 \
    node2:/data/brick1

# 启动卷
gluster volume start data

# 客户端挂载
mount -t glusterfs node1:/data /mnt
```

### 3.3 Ceph FS 配置
```bash
# 安装 Ceph
yum install epel-release
yum install ceph-deploy

# 初始化集群
ceph-deploy new node1 node2 node3

# 安装 Ceph
ceph-deploy install node1 node2 node3

# 初始化监视器
ceph-deploy mon create-initial

# 部署 OSD
ceph-deploy osd create node1:/dev/sdb
ceph-deploy osd create node2:/dev/sdb
ceph-deploy osd create node3:/dev/sdb

# 部署 MDS
ceph-deploy mds create node1

# 创建文件系统
ceph osd pool create cephfs_data 128
ceph osd pool create cephfs_metadata 128
ceph fs new cephfs cephfs_metadata cephfs_data

# 客户端挂载
mount -t ceph node1:6789:/ /mnt -o name=admin,secret=AQC1234567890
```

## 4. 对象存储虚拟化

### 4.1 Ceph RGW 配置
```bash
# 安装 RGW
ceph-deploy install --rgw node1

# 创建 RGW 实例
ceph-deploy rgw create node1

# 创建用户
radosgw-admin user create --uid="user1" \
    --display-name="First User" \
    --email=user1@example.com

# 配置 S3 客户端
cat > ~/.s3cfg << EOF
[default]
access_key = YOUR_ACCESS_KEY
secret_key = YOUR_SECRET_KEY
host_base = node1:7480
host_bucket = node1:7480
EOF
```

### 4.2 MinIO 配置
```bash
# 安装 MinIO
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio
mv minio /usr/local/bin/

# 创建存储目录
mkdir -p /data/minio

# 配置服务
cat > /etc/systemd/system/minio.service << EOF
[Unit]
Description=MinIO
Documentation=https://docs.min.io
Wants=network-online.target
After=network-online.target

[Service]
User=minio
Group=minio
Environment="MINIO_ROOT_USER=admin"
Environment="MINIO_ROOT_PASSWORD=password"
ExecStart=/usr/local/bin/minio server /data/minio
Restart=always

[Install]
WantedBy=multi-user.target
EOF

# 启动服务
systemctl enable minio
systemctl start minio
```

## 5. 存储性能优化

### 5.1 系统优化
```bash
# IO 调度器优化
echo deadline > /sys/block/sda/queue/scheduler

# 预读优化
blockdev --setra 16384 /dev/sda

# 文件系统优化
mount -o noatime,nodiratime /dev/sda1 /data

# 系统参数优化
cat >> /etc/sysctl.conf << EOF
vm.dirty_background_ratio = 5
vm.dirty_ratio = 10
vm.swappiness = 10
EOF

sysctl -p
```

### 5.2 缓存优化
```bash
# 配置 bcache
make-bcache -B /dev/sdb
make-bcache -C /dev/sdc
echo /dev/sdc > /sys/block/bcache0/bcache/attach

# 配置 dm-cache
pvcreate /dev/sdb
vgcreate vg_cache /dev/sdb
lvcreate -L 10G -n cache_meta vg_cache
lvcreate -L 100G -n cache_data vg_cache
lvconvert --type cache-pool --poolmetadata vg_cache/cache_meta vg_cache/cache_data
lvconvert --type cache --cachepool vg_cache/cache_data vg_data/lv_data
```

## 6. 高可用配置

### 6.1 多路径配置
```bash
# 安装多路径工具
yum install device-mapper-multipath

# 配置多路径
cat > /etc/multipath.conf << EOF
defaults {
    user_friendly_names yes
    path_grouping_policy multibus
    failback immediate
    no_path_retry fail
}

blacklist {
    devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*"
    devnode "^hd[a-z]"
}
EOF

# 启动服务
systemctl enable multipathd
systemctl start multipathd
```

### 6.2 存储复制
```bash
# DRBD 同步配置
drbdadm status
drbdadm connect data
watch cat /proc/drbd

# Ceph 复制配置
ceph osd pool set data size 3
ceph osd pool set data min_size 2
```

## 7. 监控与维护

### 7.1 性能监控
```bash
# 监控脚本
cat > /usr/local/bin/storage_monitor.sh << EOF
#!/bin/bash

LOG_FILE="/var/log/storage_monitor.log"
ALERT_EMAIL="admin@example.com"

# 检查存储状态
check_storage() {
    # 检查磁盘使用率
    df -h | awk '{ print \$5 " " \$6 }' | while read output; do
        usage=\$(echo \$output | awk '{ print \$1 }' | sed 's/%//g')
        partition=\$(echo \$output | awk '{ print \$2 }')
        
        if [ \$usage -ge 90 ]; then
            echo "[WARNING] \$partition usage is \$usage%" >> \$LOG_FILE
            echo "\$partition usage is \$usage%" | mail -s "Storage Alert" \$ALERT_EMAIL
        fi
    done
    
    # 检查 IO 等待
    iostat -x 1 1 | awk '/^[s|h]d[a-z]/ { if (\$10 > 80.0) print \$1 " " \$10 }' | while read output; do
        device=\$(echo \$output | awk '{ print \$1 }')
        wait=\$(echo \$output | awk '{ print \$2 }')
        
        echo "[WARNING] \$device IO wait is \$wait%" >> \$LOG_FILE
        echo "\$device IO wait is \$wait%" | mail -s "IO Alert" \$ALERT_EMAIL
    done
}

# 检查复制状态
check_replication() {
    # 检查 DRBD 状态
    if command -v drbdadm &> /dev/null; then
        if ! drbdadm status | grep -q "UpToDate/UpToDate"; then
            echo "[ERROR] DRBD not in sync!" >> \$LOG_FILE
            echo "DRBD not in sync!" | mail -s "DRBD Alert" \$ALERT_EMAIL
        fi
    fi
    
    # 检查 Ceph 状态
    if command -v ceph &> /dev/null; then
        if ! ceph health | grep -q HEALTH_OK; then
            echo "[ERROR] Ceph cluster not healthy!" >> \$LOG_FILE
            echo "Ceph cluster not healthy!" | mail -s "Ceph Alert" \$ALERT_EMAIL
        fi
    fi
}

# 主循环
while true; do
    check_storage
    check_replication
    sleep 300
done
EOF

chmod +x /usr/local/bin/storage_monitor.sh
```

### 7.2 备份策略
```bash
# 备份脚本
cat > /usr/local/bin/storage_backup.sh << EOF
#!/bin/bash

# 配置
BACKUP_ROOT="/backup"
DATE=\$(date +%Y%m%d)
RETENTION_DAYS=30

# LVM 快照备份
backup_lvm() {
    lvcreate -L10G -s -n backup_snap /dev/vg_data/lv_data
    tar -czf \$BACKUP_ROOT/lvm_\$DATE.tar.gz /dev/vg_data/backup_snap
    lvremove -f /dev/vg_data/backup_snap
}

# Ceph 备份
backup_ceph() {
    ceph osd pool ls | while read pool; do
        rados export \$pool \$BACKUP_ROOT/ceph_\${pool}_\$DATE.dump
    done
}

# 清理旧备份
cleanup() {
    find \$BACKUP_ROOT -type f -mtime +\$RETENTION_DAYS -delete
}

# 主函数
backup_lvm
backup_ceph
cleanup
EOF

chmod +x /usr/local/bin/storage_backup.sh
```

## 8. 最佳实践

### 8.1 设计建议
1. 存储架构
   - 分层存储设计
   - 性能与容量平衡
   - 冗余与备份策略
   - 扩展性考虑

2. 性能优化
   - 合理的缓存配置
   - IO 调度优化
   - 网络带宽规划
   - 监控与调优

### 8.2 运维建议
1. 日常维护
   - 定期检查性能
   - 监控存储状态
   - 及时清理垃圾
   - 更新系统补丁

2. 故障处理
   - 建立应急预案
   - 定期演练恢复
   - 故障原因分析
   - 优化改进方案
``` 