# Kubernetes集群部署配置

## 1. 基础信息
### 文档信息
- 版本: v1.0.0
- 更新时间: 2024-03-21
- 状态: [🏗️进行中]
- 作者: System Admin

### 修订历史
| 版本 | 日期 | 描述 | 作者 |
|-----|------|-----|-----|
| v1.0.0 | 2024-03-21 | 初始版本 | System Admin |

### 目标读者
- 系统运维工程师
- 平台管理员
- DevOps工程师
- 架构设计师

### 前置知识
- Linux系统管理
- 容器技术基础
- 网络基础知识
- 存储系统基础

## 2. 环境准备
### 系统要求
```bash
# 硬件配置建议
Master节点:
- CPU: 2核以上
- 内存: 4GB以上
- 硬盘: 50GB以上

Worker节点:
- CPU: 4核以上
- 内存: 8GB以上
- 硬盘: 100GB以上

# 操作系统要求
- CentOS 7.9/8.x
- Ubuntu 20.04/22.04
- 内核版本 >= 3.10
```

### 系统配置
```bash
# 关闭防火墙
systemctl stop firewalld
systemctl disable firewalld

# 关闭SELinux
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config

# 关闭swap
swapoff -a
sed -i '/swap/d' /etc/fstab

# 配置内核参数
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
vm.overcommit_memory = 1
vm.panic_on_oom = 0
fs.inotify.max_user_watches = 89100
fs.file-max = 52706963
fs.nr_open = 52706963
EOF
sysctl --system

# 配置内核模块
cat > /etc/modules-load.d/k8s.conf << EOF
br_netfilter
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
EOF
modprobe br_netfilter
modprobe ip_vs
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh
modprobe nf_conntrack

# 配置时间同步
apt install -y chrony
systemctl enable chronyd
systemctl start chronyd
chronyc sources

# 配置ulimit
cat > /etc/security/limits.d/k8s.conf << EOF
* soft nofile 65536
* hard nofile 65536
* soft nproc 65536
* hard nproc 65536
* soft memlock unlimited
* hard memlock unlimited
EOF
```

### 网络配置
```bash
# 配置hosts
cat >> /etc/hosts << EOF
192.168.1.10 k8s-master01
192.168.1.11 k8s-master02
192.168.1.12 k8s-master03
192.168.1.21 k8s-node01
192.168.1.22 k8s-node02
192.168.1.23 k8s-node03
EOF

# 配置网卡参数
cat > /etc/sysconfig/network-scripts/ifcfg-eth0 << EOF
TYPE=Ethernet
BOOTPROTO=none
NAME=eth0
DEVICE=eth0
ONBOOT=yes
IPADDR=192.168.1.10
NETMASK=255.255.255.0
GATEWAY=192.168.1.1
DNS1=8.8.8.8
EOF

# 重启网络
systemctl restart network
```

## 3. 组件安装
### 容器运行时
```bash
# 安装containerd
apt install -y containerd.io

# 配置containerd
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml

# 修改配置
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sed -i 's/registry.k8s.io/registry.aliyuncs.com\/google_containers/' /etc/containerd/config.toml

# 配置镜像加速
cat >> /etc/containerd/config.toml << EOF
[plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
  endpoint = ["https://mirror.ccs.tencentyun.com"]
EOF

# 启动服务
systemctl enable containerd
systemctl restart containerd
```

### kubeadm工具链
```bash
# 添加apt源
curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -
cat > /etc/apt/sources.list.d/kubernetes.list << EOF
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

# 安装工具
apt update
apt install -y kubelet kubeadm kubectl
systemctl enable kubelet

# 配置kubelet
cat > /etc/sysconfig/kubelet << EOF
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"
EOF
```

## 4. 集群初始化
### Master节点
```bash
# 生成初始化配置
cat > kubeadm-config.yaml << EOF
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.10
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///run/containerd/containerd.sock
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
imageRepository: registry.aliyuncs.com/google_containers
networking:
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
  dnsDomain: cluster.local
controlPlaneEndpoint: "192.168.1.10:6443"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs
EOF

# 初始化集群
kubeadm init --config=kubeadm-config.yaml --upload-certs

# 配置kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 安装网络插件(Calico)
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml
```

### Worker节点
```bash
# 加入集群
kubeadm join 192.168.1.10:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash>

# 查看节点状态
kubectl get nodes
```

## 5. 高可用配置
### HAProxy配置
```bash
# 安装HAProxy
apt install -y haproxy

# 配置HAProxy
cat > /etc/haproxy/haproxy.cfg << EOF
global
    log /dev/log    local0
    log /dev/log    local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log     global
    mode    tcp
    option  tcplog
    option  dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000

frontend kubernetes-apiserver
    bind *:6443
    mode tcp
    option tcplog
    default_backend kubernetes-apiserver

backend kubernetes-apiserver
    mode tcp
    option tcp-check
    balance roundrobin
    server k8s-master01 192.168.1.10:6443 check fall 3 rise 2
    server k8s-master02 192.168.1.11:6443 check fall 3 rise 2
    server k8s-master03 192.168.1.12:6443 check fall 3 rise 2
EOF

# 启动HAProxy
systemctl enable haproxy
systemctl restart haproxy
```

### Keepalived配置
```bash
# 安装Keepalived
apt install -y keepalived

# 配置Keepalived
cat > /etc/keepalived/keepalived.conf << EOF
global_defs {
    router_id LVS_DEVEL
    script_user root
    enable_script_security
}

vrrp_script check_haproxy {
    script "killall -0 haproxy"
    interval 3
    weight -2
    fall 10
    rise 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.1.100
    }
    track_script {
        check_haproxy
    }
}
EOF

# 启动Keepalived
systemctl enable keepalived
systemctl restart keepalived
```

## 6. 基础配置
### 命名空间管理
```bash
# 创建命名空间
kubectl create namespace monitoring
kubectl create namespace logging
kubectl create namespace ingress-nginx

# 配置资源配额
cat > quota.yaml << EOF
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: monitoring
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
EOF
kubectl apply -f quota.yaml
```

### 网络策略
```yaml
# 默认网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: monitoring
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: monitoring
```

### 存储配置
```yaml
# 本地存储类
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

### 资源限制
```yaml
# LimitRange配置
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: monitoring
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 500m
    defaultRequest:
      memory: 256Mi
      cpu: 200m
    type: Container
```

## 7. 验证测试
### 集群验证
```bash
# 检查组件状态
kubectl get componentstatuses
kubectl get pods -n kube-system

# 检查节点状态
kubectl get nodes
kubectl describe node k8s-master01

# 检查网络状态
kubectl get pods -n calico-system
kubectl get pods -A -o wide
```

### 功能测试
```bash
# 部署测试应用
kubectl create deployment nginx --image=nginx:latest
kubectl scale deployment nginx --replicas=3
kubectl expose deployment nginx --port=80 --type=NodePort

# 验证服务访问
kubectl get svc nginx
curl http://NodeIP:NodePort

# 验证Pod调度
kubectl get pods -o wide
kubectl describe pod nginx-xxxxxx
```

### 性能测试
```bash
# 压力测试
kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://nginx; done"

# 资源监控
kubectl top nodes
kubectl top pods -A
```

## 8. 故障排查
### 常见问题
1. 节点NotReady
```bash
# 检查kubelet状态
systemctl status kubelet
journalctl -u kubelet -f

# 检查容器运行时
systemctl status containerd
crictl ps
crictl pods
```

2. Pod启动失败
```bash
# 查看Pod状态
kubectl describe pod <pod-name>
kubectl logs <pod-name>

# 检查事件
kubectl get events --sort-by='.lastTimestamp'
```

3. 网络故障
```bash
# 检查网络插件
kubectl get pods -n calico-system
kubectl logs -n calico-system calico-node-xxxxx

# 检查网络连通性
kubectl exec -it <pod-name> -- ping <target-ip>
```

### 日志收集
```bash
# 收集系统日志
journalctl -u kubelet > kubelet.log
journalctl -u containerd > containerd.log

# 收集组件日志
kubectl logs -n kube-system kube-apiserver-k8s-master01 > apiserver.log
kubectl logs -n kube-system kube-controller-manager-k8s-master01 > controller-manager.log
kubectl logs -n kube-system kube-scheduler-k8s-master01 > scheduler.log

# 收集审计日志
cat /var/log/kubernetes/audit.log
```

## 9. 维护管理
### 证书更新
```bash
# 检查证书过期时间
kubeadm certs check-expiration

# 更新证书
kubeadm certs renew all

# 重启组件
systemctl restart kubelet
```

### 版本升级
```bash
# 升级kubeadm
apt update
apt install -y kubeadm=1.28.x-00

# 升级控制平面
kubeadm upgrade plan
kubeadm upgrade apply v1.28.x

# 升级kubelet
apt install -y kubelet=1.28.x-00 kubectl=1.28.x-00
systemctl daemon-reload
systemctl restart kubelet
```

### 备份恢复
```bash
# 备份etcd数据
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save snapshot.db

# 恢复etcd数据
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot restore snapshot.db
```

## 10. 最佳实践
### 安全建议
1. 控制平面安全
   - 使用TLS加密通信
   - 启用审计日志
   - 定期更新证书
   - 限制API访问

2. 节点安全
   - 及时更新系统补丁
   - 限制SSH访问
   - 监控系统日志
   - 配置防火墙规则

3. 容器安全
   - 使用最小化基础镜像
   - 定期扫描安全漏洞
   - 限制容器权限
   - 配置网络策略

### 性能优化
1. 资源配置
   - 合理设置资源限制
   - 使用资源预留
   - 配置HPA自动扩缩
   - 优化调度策略

2. 网络优化
   - 使用IPVS模式
   - 优化MTU配置
   - 配置网络QoS
   - 使用网络策略

3. 存储优化
   - 选择合适的存储类
   - 配置存储限额
   - 使用本地存储
   - 定期清理数据

### 运维建议
1. 监控告警
   - 部署监控系统
   - 配置告警规则
   - 设置告警通知
   - 定期检查监控

2. 日志管理
   - 集中化日志收集
   - 配置日志轮转
   - 设置日志保留
   - 分析日志数据

3. 备份策略
   - 定期备份数据
   - 测试恢复流程
   - 异地备份
   - 文档化操作

## 相关文档
- [核心概念组件](./02_核心概念组件.md)
- [工作负载管理](./03_工作负载管理.md)
- [服务与网络](./04_服务与网络.md)

## 参考资料
1. [Kubernetes官方文档](https://kubernetes.io/docs/)
2. [Kubernetes安装指南](https://kubernetes.io/docs/setup/)
3. [Kubernetes最佳实践](https://kubernetes.io/docs/concepts/configuration/overview/)
4. [Kubernetes集群运维](https://kubernetes.io/docs/tasks/administer-cluster/) 