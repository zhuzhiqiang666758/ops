# Ceph部署配置

## 版本信息
- 版本号: v1.0.0
- 更新日期: 2024-03-21
- 状态: [✅已完成]

## 1. 环境准备
### 1.1 硬件要求
1. 最小配置
   - CPU: 4核心/节点
   - 内存: 16GB/节点
   - 磁盘: 2块以上/OSD节点
   - 网络: 万兆网络推荐

2. 生产环境推荐
   - CPU: 8核心以上/节点
   - 内存: 32GB以上/节点
   - 磁盘: 4块以上/OSD节点
   - 网络: 万兆网络必选

### 1.2 系统要求
```bash
# 操作系统要求
- CentOS 8/RHEL 8及以上
- Ubuntu 20.04及以上
- Debian 11及以上

# 内核要求
- Linux kernel >= 4.14
- 支持RBD内核模块
- 支持CephFS内核模块
```

### 1.3 网络规划
```yaml
# 网络分离配置
public_network: "192.168.1.0/24"  # 公共网络
cluster_network: "172.16.1.0/24"  # 集群网络

# 防火墙端口
- mon: 3300/tcp, 6789/tcp
- mgr: 6800-7300/tcp
- osd: 6800-7300/tcp
- mds: 6800-7300/tcp
```

## 2. 基础安装
### 2.1 软件源配置
```bash
# RHEL/CentOS
dnf install -y epel-release
dnf install -y yum-utils
dnf config-manager --add-repo https://download.ceph.com/rpm-18.2.0/el8/noarch/

# Ubuntu/Debian
wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -
echo deb https://download.ceph.com/debian-18.2.0/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list
apt update
```

### 2.2 安装Ceph组件
```bash
# 安装cephadm
curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm
chmod +x cephadm
./cephadm add-repo --release octopus
./cephadm install

# 初始化集群
cephadm bootstrap --mon-ip 192.168.1.10
```

## 3. 集群配置
### 3.1 Monitor配置
```yaml
[mon]
mon_allow_pool_delete = true
mon_max_pg_per_osd = 300
mon_osd_down_out_interval = 600
mon_osd_min_down_reporters = 4
mon_clock_drift_allowed = 0.15
mon_clock_drift_warn_backoff = 30
```

### 3.2 OSD配置
```yaml
[osd]
osd_pool_default_size = 3
osd_pool_default_min_size = 2
osd_pool_default_pg_num = 128
osd_max_backfills = 2
osd_recovery_max_active = 3
osd_recovery_op_priority = 3
```

### 3.3 MDS配置
```yaml
[mds]
mds_cache_size = 1G
mds_cache_memory_limit = 4G
mds_max_file_size = 1099511627776
mds_log_max_segments = 128
```

## 4. 存储池配置
### 4.1 RBD存储池
```bash
# 创建存储池
ceph osd pool create rbd 128 128
ceph osd pool application enable rbd rbd

# 初始化RBD
rbd pool init rbd

# 创建RBD镜像
rbd create --size 1024 rbd/volume1
```

### 4.2 CephFS配置
```bash
# 创建文件系统
ceph osd pool create cephfs_data 128 128
ceph osd pool create cephfs_metadata 32 32
ceph fs new cephfs cephfs_metadata cephfs_data

# 部署MDS
ceph orch apply mds cephfs --placement="3 host1 host2 host3"
```

### 4.3 RadosGW配置
```yaml
# rgw配置文件
[client.rgw.gateway]
rgw_frontends = "beast port=7480"
rgw_thread_pool_size = 512
rgw_num_rados_handles = 8
rgw_override_bucket_index_max_shards = 16
```

## 5. 安全配置
### 5.1 认证配置
```bash
# 创建客户端密钥
ceph auth get-or-create client.rbd mon 'profile rbd' osd 'profile rbd pool=rbd'
ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow rw' osd 'allow rw pool=cephfs_data'
```

### 5.2 网络安全
```yaml
# 启用msgr2协议
ms_bind_msgr2: true
ms_cluster_mode: secure
ms_client_mode: secure
ms_service_mode: secure
```

## 6. 监控配置
### 6.1 Prometheus集成
```yaml
# 启用Prometheus模块
ceph mgr module enable prometheus

# 配置Prometheus endpoint
[mon]
mgr initial modules = prometheus
```

### 6.2 Grafana仪表板
```yaml
# Grafana数据源配置
name: Ceph Cluster
type: prometheus
url: http://localhost:9283
access: proxy
```

## 7. 性能调优
### 7.1 OSD性能
```yaml
# OSD性能优化
[osd]
osd_op_num_threads_per_shard = 2
osd_op_num_shards = 8
osd_memory_target = 4294967296
bluestore_rocksdb_options = "compression=kNoCompression,max_write_buffer_number=32,min_write_buffer_number_to_merge=2"
```

### 7.2 客户端性能
```yaml
# 客户端性能优化
[client]
rbd_cache = true
rbd_cache_size = 134217728
rbd_cache_max_dirty = 67108864
rbd_cache_target_dirty = 33554432
```

## 8. 维护操作
### 8.1 日常维护
```bash
# 检查集群状态
ceph health detail
ceph -s

# 检查OSD状态
ceph osd tree
ceph osd df

# 检查PG状态
ceph pg dump_stuck
```

### 8.2 扩容操作
```bash
# 添加OSD
ceph-deploy osd create --data /dev/sdb node1

# 添加MON
ceph-deploy mon add node4

# 扩展MDS
ceph mds add_data_pool cephfs_data2
```

## 9. 故障处理
### 9.1 常见问题
1. OSD故障
   ```bash
   # 标记OSD为out
   ceph osd out osd.0
   
   # 移除OSD
   ceph osd purge osd.0 --yes-i-really-mean-it
   ```

2. MON故障
   ```bash
   # 重新部署MON
   ceph-deploy mon destroy mon1
   ceph-deploy mon add mon1
   ```

### 9.2 数据恢复
```bash
# 启动恢复
ceph osd pool set rbd recovery_priority 5
ceph osd pool set rbd recovery_op_priority 5

# 控制恢复速度
ceph tell osd.* injectargs '--osd-max-backfills=1'
ceph tell osd.* injectargs '--osd-recovery-max-active=1'
```

## 参考资料
1. Ceph官方部署指南
2. Ceph性能优化手册
3. Ceph运维实践指南 