# 日志分析工具

## 版本信息
- 版本号: v1.0.0
- 更新日期: 2024-03-21
- 状态: [✅] 已完成

## 一、分析平台
### 1.1 ELK Stack
1. Elasticsearch
   ```yaml
   # Elasticsearch查询示例
   GET logs-*/_search
   {
     "query": {
       "bool": {
         "must": [
           { "match": { "level": "ERROR" }},
           { "range": {
               "@timestamp": {
                 "gte": "now-1h",
                 "lt": "now"
               }
             }}
         ]
       }
     },
     "aggs": {
       "error_by_service": {
         "terms": {
           "field": "service.keyword",
           "size": 10
         }
       }
     }
   }
   ```

2. Kibana
   ```yaml
   # Kibana仪表板配置
   {
     "title": "应用监控仪表板",
     "panels": [
       {
         "title": "错误趋势",
         "type": "line",
         "datasource": "elasticsearch",
         "targets": [
           {
             "query": "level:ERROR",
             "metrics": [
               { "type": "count" }
             ],
             "timeField": "@timestamp",
             "interval": "1m"
           }
         ]
       },
       {
         "title": "服务状态",
         "type": "gauge",
         "datasource": "elasticsearch",
         "targets": [
           {
             "query": "level:ERROR",
             "metrics": [
               { "type": "count" }
             ],
             "timeField": "@timestamp"
           }
         ]
       }
     ]
   }
   ```

### 1.2 Grafana
1. 数据源配置
   ```yaml
   # Grafana数据源
   datasources:
     - name: Elasticsearch
       type: elasticsearch
       access: proxy
       url: http://elasticsearch:9200
       database: logs-*
       jsonData:
         timeField: "@timestamp"
         interval: Daily
         esVersion: 7.0.0
     
     - name: Prometheus
       type: prometheus
       access: proxy
       url: http://prometheus:9090
       jsonData:
         timeInterval: 30s
   ```

2. 仪表板配置
   ```yaml
   # Grafana仪表板
   dashboard:
     title: "日志分析仪表板"
     panels:
       - title: "错误统计"
         type: graph
         datasource: Elasticsearch
         targets:
           - query: 'level:ERROR'
             metrics:
               - type: count
             timeField: "@timestamp"
             interval: 1m
       
       - title: "响应时间"
         type: heatmap
         datasource: Prometheus
         targets:
           - expr: 'http_request_duration_seconds'
             format: time_series
             interval: 1m
   ```

## 二、分析方法
### 2.1 实时分析
1. 日志流处理
   ```python
   from pyspark.streaming import StreamingContext
   from pyspark.streaming.kafka import KafkaUtils
   
   def analyze_log_stream():
       """
       实时日志分析
       """
       ssc = StreamingContext(sc, 10)  # 10秒批处理间隔
       
       # 创建Kafka流
       kafka_stream = KafkaUtils.createDirectStream(
           ssc, 
           topics=['logs'],
           kafkaParams={
               'bootstrap.servers': 'kafka:9092',
               'group.id': 'log-analyzer',
               'auto.offset.reset': 'latest'
           }
       )
       
       # 解析日志
       parsed_logs = kafka_stream.map(lambda x: json.loads(x[1]))
       
       # 错误统计
       error_counts = parsed_logs \
           .filter(lambda log: log['level'] == 'ERROR') \
           .map(lambda log: (log['service'], 1)) \
           .reduceByKey(lambda a, b: a + b)
       
       # 性能分析
       response_times = parsed_logs \
           .filter(lambda log: 'response_time' in log) \
           .map(lambda log: (log['service'], log['response_time'])) \
           .mapValues(lambda x: (x, 1)) \
           .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \
           .mapValues(lambda x: x[0] / x[1])
       
       # 输出结果
       error_counts.pprint()
       response_times.pprint()
       
       ssc.start()
       ssc.awaitTermination()
   ```

2. 告警处理
   ```python
   def process_alerts(stream):
       """
       实时告警处理
       """
       def check_threshold(data):
           # 错误率检查
           error_rate = data['error_count'] / data['total_count']
           if error_rate > 0.01:
               alert_error_rate(data['service'], error_rate)
           
           # 响应时间检查
           if data['response_time'] > 1000:
               alert_slow_response(data['service'], data['response_time'])
           
           # 并发量检查
           if data['concurrent_count'] > 1000:
               alert_high_concurrency(data['service'], data['concurrent_count'])
       
       def alert_error_rate(service, rate):
           alert = Alert(
               type='ERROR_RATE',
               service=service,
               value=rate,
               threshold=0.01,
               message=f'Service {service} error rate {rate:.2%} exceeds threshold'
           )
           send_alert(alert)
       
       def alert_slow_response(service, time):
           alert = Alert(
               type='SLOW_RESPONSE',
               service=service,
               value=time,
               threshold=1000,
               message=f'Service {service} response time {time}ms exceeds threshold'
           )
           send_alert(alert)
       
       def alert_high_concurrency(service, count):
           alert = Alert(
               type='HIGH_CONCURRENCY',
               service=service,
               value=count,
               threshold=1000,
               message=f'Service {service} concurrent count {count} exceeds threshold'
           )
           send_alert(alert)
       
       stream.foreachRDD(lambda rdd: rdd.foreach(check_threshold))
   ```

### 2.2 离线分析
1. 数据聚合
   ```python
   def aggregate_logs(logs, window='1h'):
       """
       日志数据聚合
       """
       def calculate_metrics(group):
           return {
               'total_count': len(group),
               'error_count': len(group[group['level'] == 'ERROR']),
               'avg_response_time': group['response_time'].mean(),
               'p95_response_time': group['response_time'].quantile(0.95),
               'success_rate': len(group[group['status'] == 'SUCCESS']) / len(group)
           }
       
       # 按时间窗口聚合
       time_groups = logs.groupby(pd.Grouper(key='timestamp', freq=window))
       time_metrics = time_groups.apply(calculate_metrics)
       
       # 按服务聚合
       service_groups = logs.groupby('service')
       service_metrics = service_groups.apply(calculate_metrics)
       
       # 按接口聚合
       api_groups = logs.groupby('api')
       api_metrics = api_groups.apply(calculate_metrics)
       
       return {
           'time_metrics': time_metrics,
           'service_metrics': service_metrics,
           'api_metrics': api_metrics
       }
   ```

2. 趋势分析
   ```python
   def analyze_trends(metrics, window='1d'):
       """
       趋势分析
       """
       def calculate_trend(series):
           # 计算移动平均
           ma = series.rolling(window=5).mean()
           
           # 计算环比变化
           change = (series - series.shift(1)) / series.shift(1)
           
           # 计算同比变化
           yoy_change = (series - series.shift(7)) / series.shift(7)
           
           # 预测未来值
           model = ARIMA(series, order=(1,1,1))
           results = model.fit()
           forecast = results.forecast(steps=5)
           
           return {
               'current': series.iloc[-1],
               'ma': ma.iloc[-1],
               'change': change.iloc[-1],
               'yoy_change': yoy_change.iloc[-1],
               'forecast': forecast.values
           }
       
       trends = {}
       
       # 错误率趋势
       trends['error_rate'] = calculate_trend(metrics['error_count'] / metrics['total_count'])
       
       # 响应时间趋势
       trends['response_time'] = calculate_trend(metrics['avg_response_time'])
       
       # 成功率趋势
       trends['success_rate'] = calculate_trend(metrics['success_rate'])
       
       return trends
   ```

## 三、分析场景
### 3.1 异常分析
1. 错误分析
   ```sql
   -- 错误分布分析
   SELECT 
     error_type,
     COUNT(*) as error_count,
     COUNT(DISTINCT service) as affected_services,
     MIN(timestamp) as first_occurrence,
     MAX(timestamp) as last_occurrence
   FROM error_logs
   WHERE timestamp >= NOW() - INTERVAL '1 day'
   GROUP BY error_type
   ORDER BY error_count DESC;
   
   -- 错误趋势分析
   WITH error_trend AS (
     SELECT 
       date_trunc('hour', timestamp) as time_slot,
       service,
       COUNT(*) as error_count
     FROM error_logs
     WHERE timestamp >= NOW() - INTERVAL '7 day'
     GROUP BY time_slot, service
   )
   SELECT 
     time_slot,
     service,
     error_count,
     LAG(error_count) OVER (PARTITION BY service ORDER BY time_slot) as prev_count,
     error_count - LAG(error_count) OVER (PARTITION BY service ORDER BY time_slot) as count_change
   FROM error_trend
   ORDER BY time_slot DESC, error_count DESC;
   ```

2. 异常检测
   ```python
   def detect_anomalies(data, window_size=24):
       """
       异常检测
       """
       def calculate_bounds(series):
           # 计算移动平均和标准差
           ma = series.rolling(window=window_size).mean()
           std = series.rolling(window=window_size).std()
           
           # 计算上下界
           upper_bound = ma + 3 * std
           lower_bound = ma - 3 * std
           
           return ma, upper_bound, lower_bound
       
       def is_anomaly(value, ma, upper, lower):
           return value > upper or value < lower
       
       # 检测错误率异常
       error_rate = data['error_count'] / data['total_count']
       er_ma, er_upper, er_lower = calculate_bounds(error_rate)
       error_anomalies = error_rate[is_anomaly(error_rate, er_ma, er_upper, er_lower)]
       
       # 检测响应时间异常
       rt_ma, rt_upper, rt_lower = calculate_bounds(data['response_time'])
       rt_anomalies = data['response_time'][is_anomaly(data['response_time'], rt_ma, rt_upper, rt_lower)]
       
       return {
           'error_rate_anomalies': error_anomalies,
           'response_time_anomalies': rt_anomalies
       }
   ```

### 3.2 性能分析
1. 接口性能
   ```sql
   -- 接口性能分析
   WITH api_stats AS (
     SELECT 
       api_path,
       COUNT(*) as request_count,
       AVG(response_time) as avg_time,
       MAX(response_time) as max_time,
       MIN(response_time) as min_time,
       PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time) as p95_time,
       PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY response_time) as p99_time
     FROM access_logs
     WHERE timestamp >= NOW() - INTERVAL '1 hour'
     GROUP BY api_path
   )
   SELECT 
     api_path,
     request_count,
     round(avg_time::numeric, 2) as avg_time_ms,
     round(p95_time::numeric, 2) as p95_time_ms,
     round(p99_time::numeric, 2) as p99_time_ms,
     round(max_time::numeric, 2) as max_time_ms,
     CASE 
       WHEN avg_time > 1000 THEN 'Critical'
       WHEN avg_time > 500 THEN 'Warning'
       ELSE 'Normal'
     END as status
   FROM api_stats
   ORDER BY avg_time DESC;
   ```

2. 资源使用
   ```sql
   -- 资源使用分析
   WITH resource_usage AS (
     SELECT 
       host,
       service,
       MAX(cpu_usage) as max_cpu,
       AVG(cpu_usage) as avg_cpu,
       MAX(memory_usage) as max_mem,
       AVG(memory_usage) as avg_mem,
       MAX(disk_usage) as max_disk,
       COUNT(*) as sample_count
     FROM metrics
     WHERE timestamp >= NOW() - INTERVAL '1 day'
     GROUP BY host, service
   )
   SELECT 
     host,
     service,
     round(avg_cpu::numeric, 2) as avg_cpu_pct,
     round(max_cpu::numeric, 2) as max_cpu_pct,
     round(avg_mem::numeric, 2) as avg_mem_pct,
     round(max_mem::numeric, 2) as max_mem_pct,
     round(max_disk::numeric, 2) as max_disk_pct,
     CASE 
       WHEN avg_cpu > 80 OR avg_mem > 80 THEN 'Critical'
       WHEN avg_cpu > 60 OR avg_mem > 60 THEN 'Warning'
       ELSE 'Normal'
     END as status
   FROM resource_usage
   ORDER BY avg_cpu DESC;
   ```

## 四、可视化方案
### 4.1 仪表板设计
1. 监控仪表板
   ```json
   {
     "title": "系统监控仪表板",
     "panels": [
       {
         "title": "系统概览",
         "type": "stat",
         "datasource": "Prometheus",
         "targets": [
           {
             "expr": "sum(up)",
             "legendFormat": "在线服务数"
           },
           {
             "expr": "sum(rate(http_requests_total[5m]))",
             "legendFormat": "请求量(QPS)"
           }
         ]
       },
       {
         "title": "错误趋势",
         "type": "graph",
         "datasource": "Elasticsearch",
         "targets": [
           {
             "query": "level:ERROR",
             "metrics": [
               { "type": "count" }
             ],
             "timeField": "@timestamp",
             "interval": "1m"
           }
         ]
       },
       {
         "title": "响应时间分布",
         "type": "heatmap",
         "datasource": "Prometheus",
         "targets": [
           {
             "expr": "rate(http_request_duration_seconds_bucket[5m])",
             "format": "heatmap",
             "legendFormat": "{{le}}"
           }
         ]
       }
     ]
   }
   ```

2. 业务仪表板
   ```json
   {
     "title": "业务监控仪表板",
     "panels": [
       {
         "title": "接口调用量",
         "type": "graph",
         "datasource": "Prometheus",
         "targets": [
           {
             "expr": "sum by (api) (rate(http_requests_total[5m]))",
             "legendFormat": "{{api}}"
           }
         ]
       },
       {
         "title": "业务成功率",
         "type": "gauge",
         "datasource": "Elasticsearch",
         "targets": [
           {
             "query": "status:SUCCESS",
             "metrics": [
               { "type": "count" }
             ],
             "timeField": "@timestamp"
           }
         ]
       },
       {
         "title": "用户活跃度",
         "type": "graph",
         "datasource": "Elasticsearch",
         "targets": [
           {
             "query": "*",
             "metrics": [
               { "type": "cardinality", "field": "user_id" }
             ],
             "timeField": "@timestamp",
             "interval": "1h"
           }
         ]
       }
     ]
   }
   ```

### 4.2 报表设计
1. 日报模板
   ```markdown
   # 系统运行日报 - {date}
   
   ## 系统状态
   1. 服务状态
      - 在线服务: {online_services}
      - 离线服务: {offline_services}
      - 告警服务: {warning_services}
   
   2. 性能指标
      - 平均响应时间: {avg_response_time}ms
      - 95分位响应时间: {p95_response_time}ms
      - 最大响应时间: {max_response_time}ms
   
   3. 错误统计
      - 错误总数: {error_count}
      - 错误率: {error_rate}%
      - 主要错误类型: {top_errors}
   
   ## 业务指标
   1. 调用量
      - 总调用量: {total_requests}
      - 平均QPS: {avg_qps}
      - 峰值QPS: {max_qps}
   
   2. 成功率
      - 整体成功率: {success_rate}%
      - 最低成功率: {min_success_rate}%
      - 失败接口TOP5: {top_failed_apis}
   
   ## 资源使用
   1. CPU使用率
      - 平均: {avg_cpu}%
      - 最大: {max_cpu}%
      - 高负载主机: {high_cpu_hosts}
   
   2. 内存使用率
      - 平均: {avg_memory}%
      - 最大: {max_memory}%
      - 高负载主机: {high_memory_hosts}
   ```

2. 周报模板
   ```markdown
   # 系统运行周报 - {week}
   
   ## 总体情况
   1. 可用性统计
      - 系统可用性: {availability}%
      - 故障次数: {incident_count}
      - 平均修复时间: {mttr}
   
   2. 性能趋势
      - 响应时间: {response_time_trend}
      - 错误率: {error_rate_trend}
      - 资源使用: {resource_usage_trend}
   
   ## 问题分析
   1. 主要问题
      - 问题1: {issue_1}
      - 问题2: {issue_2}
      - 问题3: {issue_3}
   
   2. 改进建议
      - 建议1: {suggestion_1}
      - 建议2: {suggestion_2}
      - 建议3: {suggestion_3}
   
   ## 下周计划
   1. 优化计划
      - 计划1: {plan_1}
      - 计划2: {plan_2}
      - 计划3: {plan_3}
   ```

## 相关文档
- [日志架构设计](01_日志架构设计.md)
- [日志收集方案](02_日志收集方案.md)
- [日志最佳实践](04_日志最佳实践.md)

## 更新记录
- 2024-03-21: 创建日志分析工具文档 